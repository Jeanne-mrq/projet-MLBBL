```{r}
library(readr)
library(GGally)
```

# Importation des données

```{r}
path = "https://raw.githubusercontent.com/Jeanne-mrq/projet-MLBBL/refs/heads/main/healthcare_synthetic_data.csv"
data <- read.csv(path)
```

# Analyse Exploratoire

**1. Commencez par vérifier la nature des différentes variables et leur encodage. N’oubliez pas de convertir toutes les variables qualitatives.**

```{r}
print(str(data))

#Mise en forme quanti/quali          
data$Gender <- as.factor(data$Gender)
data$Smoking_Status <- as.factor(data$Smoking_Status)
data$Family_History <- as.factor(data$Family_History)
data$Physical_Activity_Level <- as.factor(data$Physical_Activity_Level)
data$Alcohol_Consumption <- as.factor(data$Alcohol_Consumption)
data$Heart_Disease_Risk <- as.factor(data$Heart_Disease_Risk)
data$Sleep_Hours <- as.factor(data$Sleep_Hours)
data$Stress_Level <- as.factor(data$Stress_Level)

print(str(data))
```

**2. Commencez l’exploration par une analyse descriptive unidimensionnelle des données. Des transformations des variables quantitatives vous semblent-t-elles pertinentes ?**

```{r}

library(ggplot2)
library(gridExtra)

plot_quantitative <- function(df) {
  vars_num <- names(df)[sapply(df, is.numeric)]
  
  plots <- lapply(vars_num, function(v) {
    ggplot(df, aes(x = .data[[v]])) +
      geom_histogram(aes(y = after_stat(density)),
                     bins = 30,
                     fill = "grey80",
                     color = "black") +
      geom_density(alpha = 0.2, color = "blue") +
      labs(title = v, x = v, y = "Density") +
      theme_minimal()
  })
  
  do.call(grid.arrange, c(plots, ncol = 3))

}



plot_quantitative(data)

```

Certaines data ne sont pas symétriques, on applique un log

```{r}
data[, "LogWeight_kg"] <- log(data[, "Weight_kg"])
data[, "LogBMI"] <- log(data[, "BMI"])
data[, "LogHeight_cm"] <- log(data[, "Height_cm"])

# Supprimer plusieurs colonnes à la fois
data <- data[, !colnames(data) %in% c("Weight_kg", "BMI", "Height_cm")]
plot_quantitative(data)


# Colonnes quantitatives
quanti_cols <- c("Age","LogHeight_cm","LogWeight_kg","LogBMI",
                 "Systolic_BP","Diastolic_BP",
                 "Cholesterol_Total","Cholesterol_LDL",
                 "Cholesterol_HDL","Fasting_Blood_Sugar")

# Colonnes qualitatives
quali_cols <- c("Gender","Smoking_Status","Alcohol_Consumption",
                "Physical_Activity_Level","Family_History",
                "Stress_Level","Sleep_Hours","Heart_Disease_Risk")
```

**3. Poursuivez avec une analyse descriptive bidimensionnelle. Utilisez des techniques de visualisation: par exemple les nuages de points (scatterplot), des graphes des corrélations, des boîtes à moustaches parallèles,mosaicplot... Quelles variables semblent liées ?**

```{r}
library(tidyr)
library(dplyr)

# Variables quantitatives
quanti_cols <- c("Age",
                 "Systolic_BP","Diastolic_BP",
                 "Cholesterol_Total","Cholesterol_LDL",
                 "Cholesterol_HDL","Fasting_Blood_Sugar")

# Convertir en format long
data_long <- data %>%
  select(all_of(c("Heart_Disease_Risk", quanti_cols))) %>%
  pivot_longer(cols = all_of(quanti_cols),
               names_to = "Variable",
               values_to = "Value")

# Boxplots côte à côte pour chaque variable
ggplot(data_long, aes(x = Variable, y = Value, fill = Heart_Disease_Risk)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +  # <--- boxplots côte à côte
  scale_fill_manual(values = c("0" = "lightblue", "1" = "salmon"), labels = c("No", "Yes")) +
  labs(x = "Variable", y = "Value", fill = "Heart Disease Risk",
       title = "Distribution des variables quantitatives selon Heart Disease Risk") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


for (col in quali_cols) {
  p <- ggplot(data, aes_string(x = col, fill = "Heart_Disease_Risk")) +
    geom_bar(position = "fill") +
    ylab("Proportion") +
    ggtitle(paste("Proportion de risque de maladie cardiaque par", col)) +
    theme_minimal()
  print(p)
}

```

```{r}
vars_num <- names(data)[sapply(data, is.numeric)]

```

```{r}

vars_num <- names(data)[sapply(data, is.numeric)]

ggpairs(
  data[, c(vars_num, "Heart_Disease_Risk")],
  mapping = aes(alpha = 0.3, color = Heart_Disease_Risk)
) 

library(corrplot)
corrplot(cor(data[, vars_num]),method="ellipse")
```

**4. Réalisez une analyse en composantes principales des variables explicatives quantitatives et interprétez les**

résultats. Visualisez les dépendances éventuelles entre les variables à prédire et les variables explicatives.

```{r}

library(FactoMineR)




acp <- PCA(data[, c(quanti_cols, quali_cols)],
           scale.unit = TRUE,
           graph = FALSE,
           quali.sup = length(quanti_cols) + seq_along(quali_cols),
           ncp = 7)



# Décroissance des valeurs propres

library(gridExtra)
library(factoextra)
g1<-fviz_eig(acp, addlabels = TRUE, ylim = c(0, 40))
library(reshape2)
g2<-ggplot(melt(acp$ind$coord),aes(x=Var2,y=value))+
  geom_boxplot()+
  xlab("")
grid.arrange(g1,g2,ncol=2)
# 
library(corrplot)
corrplot(acp$var$cor, is.corr=FALSE,method="ellipse")
```

```{r}
 fviz_pca_var(acp)
fviz_pca_ind(acp,col.ind="contrib",label="none",gradient.cols = c("white", "#2E9FDF", "#FC4E07" ), repel = TRUE)
fviz_pca_var(acp,axes=c(1,2), repel = TRUE)

fviz_pca_ind(acp,col.ind="contrib",label="none",gradient.cols = c("white", "#2E9FDF", "#FC4E07" ),axes=c(1,3), repel = TRUE)
fviz_pca_var(acp,axes=c(1,3), repel = TRUE)

```

# Modélisation

1.  Divisez le jeu de données en un échantillon d’apprentissage et un échantillon test. Vous prendrez un pourcentage de 20% pour l’échantillon test. Pourquoi cette étape est-elle nécessaire lorsque nous nous concentrons sur les performances des algorithmes ?

```{r}
quanti_cols <- c("Age","LogHeight_cm","LogWeight_kg","LogBMI",
                 "Systolic_BP","Diastolic_BP",
                 "Cholesterol_Total","Cholesterol_LDL",
                 "Cholesterol_HDL","Fasting_Blood_Sugar")
library(caret)
trainIndex<-createDataPartition(data[,16],1,p=0.8, list=FALSE)

trainData<-data[trainIndex,]
testData<-data[-trainIndex,]
```

```{r}

```

```{r}
params<-preProcess(trainData,method=c("center","scale"))
TrainCenter<-predict(params,trainData)

TestCenter<-predict(params,testData)

TrainCenter<-TrainCenter[,-1]
TestCenter<- TestCenter[,-1]
summary(TrainCenter)
summary(TestCenter)
```

```{r}
gplot.res <- function(x, y, titre = "titre"){
    ggplot(data.frame(x=x, y=y),aes(x,y))+
    geom_point(col = "blue")+xlim(0, 250)+ylim(-155, 155)+
    ylab("Résidus")+ xlab("Valeurs prédites")+
    ggtitle(titre)+
    geom_hline(yintercept = 0,col="green")
}
```

## Modélisation modèle Gaussien

2.  Comparez les performances d’un modèle linéaire (éventuellement généralisé) avec/sans sélection de vari- ables, avec/sans pénalisatio

```{r}
# estimation du modèle sans interaction
# Utiliser lm() pour une régression linéaire multiple
reg.logit <- glm(Heart_Disease_Risk ~ .*. , 
                 data = TrainCenter, 
                 family = "binomial")


# Extraction (identique à votre code)
res.lm <- residuals(reg.lm)
fit.lm <- fitted(reg.lm)

# Afficher le résumé complet (Coefficients, R², p-values)
summary(reg.logit)
```

```{r}
# Tracer les 4 graphiques de diagnostic standard
par(mfrow = c(2, 2))
plot(reg.logit)
```

```{r}
# Prévision du modèle quadratique
pred.log <- predict(reg.logit, newdata = TestCenter, type = "response")
# 1. On transforme les probabilités en classes (Seuil à 0.5)
pred.class <- ifelse(pred.log > 0.5, "Yes", "No") 

# 2. Création de la matrice de confusion (Base R)
conf_matrix <- table(Prédiction = pred.class, Réel = TestCenter$Heart_Disease_Risk)
print(conf_matrix)
```

```{r}
n <- nrow(TrainCenter)

# Sélection descendante basée sur le BIC
model_bic <- step(reg.logit, 
                  direction = "backward", 
                  k = log(n), 
                  trace = 0)

# Voir le modèle final retenu
summary(model_bic)
```

```{r}
# Prévision du modèle quadratique
pred.log <- predict(model_bic, newdata = TestCenter, type = "response")
# 1. On transforme les probabilités en classes (Seuil à 0.5)
pred.class <- ifelse(pred.log > 0.5, "Yes", "No") 

# 2. Création de la matrice de confusion (Base R)
conf_matrix <- table(Prédiction = pred.class, Réel = TestCenter$Heart_Disease_Risk)
print(conf_matrix)
```

```{r}
library(glmnet)

# Correction du code pour une classification (Lasso Logistique)
x.mat <- model.matrix(Heart_Disease_Risk ~ . - 1, data = TrainCenter)
y.vec <- TrainCenter$Heart_Disease_Risk

# L'argument family = "binomial" est CRUCIAL ici


reg.lasso <- glmnet(x = x.mat, 
                    y = y.vec, 
                    family = "binomial") 

# On relance le plot
plot(reg.lasso, xvar = "lambda", label = TRUE)
legend("topright", 
legend = paste(1:ncol(x.mat), " - ", colnames(x.mat)))
```

```{r}
library(ggfortify)
library(ggplot2)

reg.lasso.cv <- cv.glmnet(y = TrainCenter[, 15], x = x.mat, family = "binomial")
#plot(reg.lasso.cv)
autoplot(reg.lasso.cv)

# valeur estimée
paste("CV estimate of lambda :", round(reg.lasso.cv$lambda.1se, 20))
# modèle correspondant
coef(reg.lasso.cv, s = "lambda.1se")

```

```{r}
gplot.res <- function(x, y, titre = "titre"){
    ggplot(data.frame(x=x, y=y),aes(x,y))+
    geom_point(col = "blue")+xlim(0, 250)+ylim(-155, 155)+
    ylab("Résidus")+ xlab("Valeurs prédites")+
    ggtitle(titre)+
    geom_hline(yintercept = 0,col="green")
}


fit.lasso <- predict(reg.lasso.cv, s = "lambda.min", newx = x.mat)
res.lasso <- TrainCenter$Heart_Disease_Risk - fit.lasso

fit.lasso.1se <- predict(reg.lasso.cv, s = "lambda.1se", newx = x.mat)
res.lasso.1se <- TrainCenter$Heart_Disease_Risk - fit.lasso.1se 

# Graphe des résidus
options(repr.plot.width = 12, repr.plot.height = 4)
par(mfrow = c(1, 3))
gplot.res(fit.lm, res.lm, "Linéaire, sans sélection")
gplot.res(fit.lasso, res.lasso, "Linéaire, pénalité L1, lambda min")
gplot.res(fit.lasso.1se, res.lasso.1se, "Linéaire, pénalité L1, lambda 1se") 
```
